### ナイーブベイズ
ナイーブベイズ（Naive Bayes）分類器は、条件付き確率に基づいた単純かつ強力な分類アルゴリズムです。ベイズの定理を利用し、各特徴が他の特徴から独立していると仮定しているため、「ナイーブ（単純）」と呼ばれます。

#### ベイズの定理
ベイズの定理は、次のように表されます：
$P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}$
- $ P(C|X) $: 事後確率。特徴Xが与えられたときのクラスCの確率
- $ P(X|C) $: 尤度。クラスCが与えられたときの特徴Xの確率
- $ P(C) $: 事前確率。クラスCの確率
- $ P(X) $: 証拠。特徴Xの確率

#### ナイーブベイズの手順
1. **学習**: トレーニングデータを使用して、各クラスの事前確率 $ P(C) $ および各特徴の尤度 $ P(X|C) $ を計算します。
2. **分類**: 新しいデータポイントが与えられたとき、各クラスの事後確率 $ P(C|X) $ を計算し、最大の事後確率を持つクラスを予測クラスとして選択します。

### ロジスティック回帰
ロジスティック回帰（Logistic Regression）は、二項分類問題を解くための統計的手法で、線形回帰を拡張して0から1の間の確率を出力します。

#### ロジスティック関数（シグモイド関数）
ロジスティック回帰は、シグモイド関数を使用して、出力を確率に変換します：
$\sigma(z) = \frac{1}{1 + e^{-z}}$
ここで、$ z = \mathbf{w}^T \mathbf{x} + b $
- $ \mathbf{w} $: 重みベクトル
- $ \mathbf{x} $: 特徴ベクトル
- $ b $: バイアス項

#### ロジスティック回帰の手順
1. **モデルの定義**: シグモイド関数を使用して、入力特徴の線形結合を確率に変換します。
2. **損失関数の定義**: ロジスティック回帰では、クロスエントロピー損失（または対数損失）を最小化します：
$\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y_i}) + (1 - y_i) \log(1 - \hat{y_i})]$
ここで、$ y_i $ は実際のラベル、$ \hat{y_i} $ は予測確率です。
3. **最適化**: 重み $ \mathbf{w} $ およびバイアス $ b $ を最適化するために勾配降下法などの最適化アルゴリズムを使用します。
4. **予測**: 学習済みモデルを使用して、新しいデータポイントのクラスを予測します。シグモイド関数の出力が0.5以上の場合はクラス1、それ以下の場合はクラス0とします。

