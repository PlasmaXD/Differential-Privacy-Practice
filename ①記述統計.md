<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>



# 分析ケース①: 記述統計へのDP適用


### 1. データの前処理
データの前処理として、クチコミのテキストデータに形態素解析を行い、各クチコミをBag-of-Words（BoW）として表現
#### BoW例
- 特定の品詞（形容詞・副詞・間投詞・名詞・固有名詞・動詞）を抽出
- 抽出した単語を用いてBoWを生成し、クチコミをベクトル表現に変換  

① 「ここのスタッフは非常に親切でした。」  
②「 トイレが少し汚かったのが残念なところです。 」  
③「ここのスタッフは非常に親切でした。ただ、トイレが少し汚かったのが残念なところです。」  

という3つの文章があった場合、まずこれを単語に分割して、出現回数を計算　　
|   | ここ | スタッフ | トイレ | 非常に | 少し | 親切な | です | 汚い | 残念な |
|---|---|---|---|---|---|---|---|---|---|
| ① | 1 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 0 |
| ② | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 1 |
| ③ | 1 | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 1 |



<!-- #### 数式
クチコミ $d_i$に対して形態素解析を行い、単語の集合 
$T(d_i)$
を得る。
$ T(d_i) = \{ t_1, t_2, \ldots, t_n \} $
ここで、$t_i$はクチコミ $d_i$に含まれる単語です。 -->

### 2. 単語の出現頻度の計算
各クラス（positive, neutral, negative）のクチコミについて、BoWを用いて単語の出現頻度を計算

- 各クラスごとにBoWを生成し、単語の出現頻度を計算
![alt text](image-11.png)
<!-- ![alt text](image-10.png)
![alt text](image-12.png) -->

- 各クラスの上位単語を抽出



<!-- #### 数式
各クラス $C_k$に対してBoWを生成し、出現頻度ベクトル $f_k$を計算  
$f_k = \left( \sum_{d_i \in C_k} \text{BoW}(d_i) \right) $ -->

### 3. 差分プライバシーの適用
Laplaceメカニズムを用いて、出現頻度にノイズを加えます。これにより、個々のクチコミデータのプライバシーを保護

- 各単語の出現頻度に対してLaplaceノイズを加える
<!-- #### 数式 -->




<!-- 出現頻度 $f_k$→出現頻度にノイズを加えた頻度 $f_k$  
$f_k' = f_k + \text{Lap}(\frac{1}{\epsilon}) $  
※$\text{Lap}(\frac{1}{\epsilon})$はスケールパラメータ $\frac{1}{\epsilon}$のLaplace分布からのサンプルです。 -->



<!-- ### 具体例と数式の説明

#### データの読み込みと形態素解析の実施
```python
df["doc"] = [nlp(review) for review in df["review"]]
```
ここでは、各クチコミに対して形態素解析を行い、形態素解析済みのクチコミを `doc` 列に格納しています。 -->

<!-- #### BoWの生成と単語の出現頻度の計算
```python
bows = {}
cvs = {}
for sentiment in df["sentiment"].unique():
    tokens = []
    for doc in df[df["sentiment"] == sentiment]["doc"]:
        similarities = [(token.similarity(doc), token.lemma_) for token in doc if token.pos_ in POS]
        similarities = remove_duplicates(similarities)
        similarities = sorted(similarities, key=lambda sim: sim[1], reverse=True)[:MAX_TERMS_IN_DOC]
        tokens.append([similaritity[1] for similaritity in similarities])
    cv = CountVectorizer(ngram_range=(1, NGRAM), max_df=MAX_DF, min_df=MIN_DF, max_features=NUM_VOCAB)
    bows[sentiment] = cv.fit_transform(flatten(tokens)).toarray()
    cvs[sentiment] = cv
```
各クラス（positive, neutral, negative）ごとにBoWを生成し、単語の出現頻度を計算しています。

#### 差分プライバシーの適用
```python
from pydp.algorithms.laplacian import Count

def preprocess_for_private_counts(tf: np.ndarray) -> List[np.ndarray]:
    repeated_words = []
    for i, term in enumerate(tf):
        repeated_words.append(np.repeat(i, term))
    return repeated_words

def cal_private_count(
    epsilon: float,
    max_partition_contributed: float,
    max_contributions_per_partition: float,
    repeated_words: List[np.ndarray],
) -> List[int]:
    private_counts = []
    for repeated_word in repeated_words:
        counter = Count(epsilon, max_partition_contributed, max_contributions_per_partition)
        count = counter.quick_result(repeated_word)
        private_counts.append(count)
    return private_counts
``` -->
## 通常の結果

![alt text](image.png)

![alt text](image-1.png)

![alt text](image-2.png)



## クランプ処理
### 目的：極端な値が推定値に大きな影響を与えるのを防ぐ  

$
推定値_{clamped} = 
\begin{cases} 
\Lambda & (推定値 > \Lambda) \\
-\Lambda & (推定値 < -\Lambda) \\
推定値 & \text{otherwise} 
\end{cases} 
$

## クランプをした結果

![alt text](image-3.png)

![alt text](image-4.png)

![alt text](image-5.png)